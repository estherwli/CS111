NAME: Luca Matsumoto
ID: 204726167
EMAIL: lucamatsumoto@gmail.com

lab2_list.c: C source file for that implements the command line options --iterations, --threads, --yield, --sync, and --lists. This program allows the specified number of threads to operate on a shared link list with the ability to split the list into a specified number of sublists. 

SortedList.h: header file containing the interfaces for the specified linked list operations

SortedList.c: C source module that implements the linked list operations insert, delete, lookup, and length

Makefile: Makefile to build the components necessary for submission. The default target creates the lab2_list executable, the tests target runs the specified tests to create the CSV file, the profile target runs tests with profiling tools to generate an execution profiling report, the graphs target uses gnuplot to generate the required graphs, the dist file generates the tarball for submission, the clean target deletes all programs and outputs generated by Makefile.

lab2b_list.csv: CSV file that stores the results from the test runs

profile.out: execution profiling report showing the amount of time spent in particular operations in spin-lock implementation

tester.sh: Testing script that runs all of the tests needed for the lab

lab2b_1.png: Graph of the throughput vs number of threads for mutex and spin-lock synchronized operations

lab2b_2.png: Graph of the mean time per mutex wait and mean time per operation wait for mutex synchronized operations

lab2b_3.png: Graph of the successful iterations vs threads for mutex and spin lock synchronized methods

lab2b_4.png: Graph of the throughput vs the number of threads for mutex-synchronized paritioned lists

lab2b_5.png: Graph of the throughput vs the number of threads for spin-lock synchronized partitioned lists

lab2_list.gp: Script that generates the graphs needed for submission using gnuplot

README: contains descriptions of the files submitted and the answers to the lab questions


Questions

2.3.1
For 1 thread with mutex, most of the time likely goes to the list operations themselves because the time that it takes to acquire/release the lock is less than the list operations itself.
For 2 threads with mutex, most of the time still goes towards the list operations because the time it takes for locking and unlocking is less than the list operations for large list
For 1 thread with spin lock, most of the time is spent on executing the list opeartions.
For 2 threads with spin lock, there is more time spent on locking/unlocking because while one thread is executing the list operations, the other one is spinning. 

2.3.2
The line of code with while(__sync_lock_test_and_set(&spinLock, 1)) seems to be consuming the most of the cycles when the spin lock version is run with a large number of threads. This code grabs the spin lock and will be run in a while loop.
This operation is expensive with large number of threads because there will be higher contention for the lock, meaning that there will be more threads spinning while waiting for the lock.

2.3.3
The average lock wait time rises dramatically with increasing number of threads due to the high contention for the lock. This means there are more threads that are waiting to grab the lock and run the mutex protected operation.
The completion time per operation also rises (less dramatically) because there are more list operations to be executed, meaning that the amount of threads that need to be put to sleep and waken up increase. As with avergae lock wait time, the total completion time also increases due to the time spent on context switches and other overheads. However, it is less dramatic because it is calculated as an overall value after all threads have joined the main thread (finish execution), while the average lock wait time is calculated per thread, meaning that the average lock wait time increases more dramatically.
As explained above, the completion time is the mean amount of time per operation, while the wait-for-lock time is the average amount of time per thread. (not total). Because the wait times for threads can overlap with each other, the average wait time per lock could increase more dramatically than the completion time. 

2.3.4
Looking at the graphs, it is clear that the performance of the partitioned list improves with the number of sublists increasing. This is because there is less contention because instead of all threads waiting for one lock, threads now have the ability to acquire locks from different lists, lowering contention and improving performance. If a thread wants to acquire a lock, it has to compete with a smaller number of threads compared to when there is only one lock.
The throughput should increase asymptotically, meaning that it will increase until a certain point. Eventually, we can have minimal contention by having each thread have its own lock so that very few threads would be waiting on each lock. Additionally, increasing the number of lists past this point could decrease throughput because the number of elements in each list would be very small, meaning that the total number of lock operations continues to increase without increasing parallelism (already at its max).
The throughput of an N-way partitioned list seems to be equivalent to a single list with fewer (1/N) threads. However, this may not always be the case the list lengths of each sublist would be different with increasing number of sublists. This would make the critical sections different and thus contention would be different. Therefore, the throughput of an N-way partitioned list would not necessarily be the same as the throughput of a single list with 1/N threads. 

Resources used
https://computing.llnl.gov/tutorials/pthreads/ for pthread help
http://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Atomic-Builtins.html help with spin locks 
https://www.cs.rutgers.edu/~pxk/416/notes/c-tutorials/gettime.html for clock_gettime help
https://gperftools.github.io/gperftools/cpuprofile.html: for profiling tool tutorial
